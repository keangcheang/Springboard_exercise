With regard to the capstone project which is about feature selections of important attributes that determine house price in Nashville, there were some data wrangling techniques used to prepare the data and ready for modeling. 

First of all, checking the format of each feature regarding their datatypes and their observation sizes was my first step. This way I would know if there should be any datatype conversions I should take or any missing values I had to tackle. In this dataset, there were 56,000+ observations with 29 variables and 31 columns.  There were several features that contained missing values such as the exterior wall, tax district â€¦etc. The house price attribute was numeric as well as land value, building value and total value which were essential to this project. 

Next, I had to check the distribution of price variable whether it was a normal distribution and make correction according to that. The result showed that the price is not normally distributed and it was hard to interpret, thus I took the log transformation of the price variable so it can be interpretable and became a bell shaped distribution with a little of skewness to the left. This indicated that some houses were sold higher than the average price which was logical and rational to the current housing market in Nashville. Moreover, total value, land value and building value were also converted to log value so that I could plot them against price in order to check their correlations. Like expected, they all had a positive relationship with price. In addition to this, I also made a heat-map with the correlation matrix to delve more into their relationship with price. Land value, total value, finished value and full bath had moderate correlations against price. 

After that, we had to deal with missing value. Firstly, to deal with missing value in categorical values like exterior wall and tax district, I had to find the most common value for these features in order to the replace the value against the missing value since forward filling or backward filling would not work in this case. For other numeric variables, I will use their means to replace the missing value. These methods could be done using function fillna() in pandas library. There were not any outliers presented in the data. 

Lastly, due to the model I used required the categorical value to be dummy variables, I had to convert the variables to binary value such 0 and 1 to different columns. I used pandas to_dummie() function to tackle this task.
